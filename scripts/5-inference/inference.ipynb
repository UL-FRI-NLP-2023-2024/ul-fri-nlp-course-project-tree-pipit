{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16621967-dbaa-40e3-bf9a-ee6dc80e55d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from peft import PeftModel\n",
    "import transformers\n",
    "import gradio as gr\n",
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5693afc1-0b70-409e-83bf-16ddf0a00371",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (\n",
    "    \"LlamaTokenizer\" in transformers._import_structure[\"models.llama\"]\n",
    "), \"LLaMA is now in HuggingFace's main branch.\\nPlease reinstall it: pip uninstall transformers && pip install git+https://github.com/huggingface/transformers.git\"\n",
    "from transformers import LlamaTokenizer, LlamaForCausalLM, GenerationConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c104eb-d3f1-4b36-ada1-5ed84a2a3b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = LlamaTokenizer.from_pretrained(\"decapoda-research/llama-7b-hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f374fca-4f00-4606-8297-6a5c2dd3dab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_MODEL = \"decapoda-research/llama-7b-hf\"\n",
    "LORA_WEIGHTS = \"tloen/alpaca-lora-7b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab0e697c-22de-44c3-afd4-0bfa7547cbdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(\n",
    "    load_8bit: bool = False,\n",
    "    base_model: str = \"\",\n",
    "    lora_weights: str = \"tloen/alpaca-lora-7b\",\n",
    "    prompt_template: str = \"\",  # The prompt template to use, will default to alpaca.\n",
    "    server_name: str = \"0.0.0.0\",  # Allows to listen on all interfaces by providing '0.\n",
    "    share_gradio: bool = False,\n",
    "):\n",
    "    base_model = base_model or os.environ.get(\"BASE_MODEL\", \"\")\n",
    "    assert (\n",
    "        base_model\n",
    "    ), \"Please specify a --base_model, e.g. --base_model='huggyllama/llama-7b'\"\n",
    "\n",
    "    prompter = Prompter(prompt_template)\n",
    "    tokenizer = LlamaTokenizer.from_pretrained(base_model)\n",
    "    if device == \"cuda\":\n",
    "        model = LlamaForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            load_in_8bit=load_8bit,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_weights,\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    elif device == \"mps\":\n",
    "        model = LlamaForCausalLM.from_pretrained(\n",
    "            base_model,\n",
    "            device_map={\"\": device},\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_weights,\n",
    "            device_map={\"\": device},\n",
    "            torch_dtype=torch.float16,\n",
    "        )\n",
    "    else:\n",
    "        model = LlamaForCausalLM.from_pretrained(\n",
    "            base_model, device_map={\"\": device}, low_cpu_mem_usage=True\n",
    "        )\n",
    "        model = PeftModel.from_pretrained(\n",
    "            model,\n",
    "            lora_weights,\n",
    "            device_map={\"\": device},\n",
    "        )\n",
    "\n",
    "    # unwind broken decapoda-research config\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id = 0  # unk\n",
    "    model.config.bos_token_id = 1\n",
    "    model.config.eos_token_id = 2\n",
    "\n",
    "    if not load_8bit:\n",
    "        model.half()  # seems to fix bugs for some users.\n",
    "\n",
    "    model.eval()\n",
    "    if torch.__version__ >= \"2\" and sys.platform != \"win32\":\n",
    "        model = torch.compile(model)\n",
    "\n",
    "    def evaluate(\n",
    "        instruction,\n",
    "        input=None,\n",
    "        temperature=0.1,\n",
    "        top_p=0.75,\n",
    "        top_k=40,\n",
    "        num_beams=4,\n",
    "        max_new_tokens=128,\n",
    "        stream_output=False,\n",
    "        **kwargs,\n",
    "    ):\n",
    "        prompt = prompter.generate_prompt(instruction, input)\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "        input_ids = inputs[\"input_ids\"].to(device)\n",
    "        generation_config = GenerationConfig(\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            top_k=top_k,\n",
    "            num_beams=num_beams,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        generate_params = {\n",
    "            \"input_ids\": input_ids,\n",
    "            \"generation_config\": generation_config,\n",
    "            \"return_dict_in_generate\": True,\n",
    "            \"output_scores\": True,\n",
    "            \"max_new_tokens\": max_new_tokens,\n",
    "        }\n",
    "\n",
    "        if stream_output:\n",
    "            # Stream the reply 1 token at a time.\n",
    "            # This is based on the trick of using 'stopping_criteria' to create an iterator,\n",
    "            # from https://github.com/oobabooga/text-generation-webui/blob/ad37f396fc8bcbab90e11ecf17c56c97bfbd4a9c/modules/text_generation.py#L216-L243.\n",
    "\n",
    "            def generate_with_callback(callback=None, **kwargs):\n",
    "                kwargs.setdefault(\n",
    "                    \"stopping_criteria\", transformers.StoppingCriteriaList()\n",
    "                )\n",
    "                kwargs[\"stopping_criteria\"].append(\n",
    "                    Stream(callback_func=callback)\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                    model.generate(**kwargs)\n",
    "\n",
    "            def generate_with_streaming(**kwargs):\n",
    "                return Iteratorize(\n",
    "                    generate_with_callback, kwargs, callback=None\n",
    "                )\n",
    "\n",
    "            with generate_with_streaming(**generate_params) as generator:\n",
    "                for output in generator:\n",
    "                    # new_tokens = len(output) - len(input_ids[0])\n",
    "                    decoded_output = tokenizer.decode(output)\n",
    "\n",
    "                    if output[-1] in [tokenizer.eos_token_id]:\n",
    "                        break\n",
    "\n",
    "                    yield prompter.get_response(decoded_output)\n",
    "            return  # early return for stream_output\n",
    "\n",
    "        # Without streaming\n",
    "        with torch.no_grad():\n",
    "            generation_output = model.generate(\n",
    "                input_ids=input_ids,\n",
    "                generation_config=generation_config,\n",
    "                return_dict_in_generate=True,\n",
    "                output_scores=True,\n",
    "                max_new_tokens=max_new_tokens,\n",
    "            )\n",
    "        s = generation_output.sequences[0]\n",
    "        output = tokenizer.decode(s)\n",
    "        yield prompter.get_response(output)\n",
    "\n",
    "    gr.Interface(\n",
    "        fn=evaluate,\n",
    "        inputs=[\n",
    "            gr.components.Textbox(\n",
    "                lines=2,\n",
    "                label=\"Instruction\",\n",
    "                placeholder=\"Tell me about alpacas.\",\n",
    "            ),\n",
    "            gr.components.Textbox(lines=2, label=\"Input\", placeholder=\"none\"),\n",
    "            gr.components.Slider(\n",
    "                minimum=0, maximum=1, value=0.1, label=\"Temperature\"\n",
    "            ),\n",
    "            gr.components.Slider(\n",
    "                minimum=0, maximum=1, value=0.75, label=\"Top p\"\n",
    "            ),\n",
    "            gr.components.Slider(\n",
    "                minimum=0, maximum=100, step=1, value=40, label=\"Top k\"\n",
    "            ),\n",
    "            gr.components.Slider(\n",
    "                minimum=1, maximum=4, step=1, value=4, label=\"Beams\"\n",
    "            ),\n",
    "            gr.components.Slider(\n",
    "                minimum=1, maximum=2000, step=1, value=128, label=\"Max tokens\"\n",
    "            ),\n",
    "            gr.components.Checkbox(label=\"Stream output\"),\n",
    "        ],\n",
    "        outputs=[\n",
    "            gr.inputs.Textbox(\n",
    "                lines=5,\n",
    "                label=\"Output\",\n",
    "            )\n",
    "        ],\n",
    "        title=\"ðŸ¦™ðŸŒ² Alpaca-LoRA\",\n",
    "        description=\"Alpaca-LoRA is a 7B-parameter LLaMA model finetuned to follow instructions. It is trained on the [Stanford Alpaca](https://github.com/tatsu-lab/stanford_alpaca) dataset and makes use of the Huggingface LLaMA implementation. For more information, please visit [the project's website](https://github.com/tloen/alpaca-lora).\",  # noqa: E501\n",
    "    ).queue().launch(server_name=\"0.0.0.0\", share=share_gradio)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
